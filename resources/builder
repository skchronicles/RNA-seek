#!/usr/bin/env bash
set -eu

function usage() { cat << EOF
builder: Job submission wrapper script for the RNA-seek Reference Building Pipeline.
USAGE:
  builder <MODE> [OPTIONS] -j MASTER_JOB_NAME -b SINGULARITY_BIND_PATHS
SYNOPSIS:
  This script submits the reference builder master job to the cluster. The master job
acts as the pipeline's main controller or its main process. This main job dictates
how subsequent jobs are submitted to the cluster via the SLURM job scheduler. Support
for additional job schedulers (i.e. PBS, SGE, LSF, Tibanna) may be added in the future.
  The main entry point of the pipeline 'rna-seek' calls this job submission wrapper script.
As so, this script can be used to by-pass 'rna-seek' for a previously failed run; meaning,
it can be used to re-run the pipeline to pick back off where the last failure occurred
or re-start the pipeline.
  Please Note: it is highly recommended to use 'rna-seek'; it is the main entry point
and preferred entry point of the RNA-seek pipeline. If you are experience error, it
maybe due to improperly mounting singularity bind paths which 'rna-seek' will internally
handle.

Required Positional Argument:
  [1] MODE  [Type: Str] Define the snakemake executor mode.
                        Valid mode options include: <slurm>
                         a) slurm: uses slurm and singularity snakemake backend.
                             The slurm EXECUTOR will submit jobs to the cluster.
                             It is recommended running RNA-seek in this mode as
                             most of the steps are computationally intensive.
Required Arguments:
  -j, --job-name [Type: Str]    Name of pipeline's master job.
  -b, --bind-paths [Type:Path]  Singularity bind paths. The RNA-seek pipeline uses
                                 singaularity images for exection. Bind paths are
                                 used to mount the host filesystem to the container's
                                 filesystem. Multiple bind paths can be provided
                                 as a comma seperated list. The main entry point
                                 of the pipeline internally collects and aggregates
                                 bindpaths to mount to the container's filesystem.
                                 If you are manually running this script or by-passing
                                 rna-seek, you will need to provide the bindpaths of
                                 the rawdata directory(s) along with the pipeline's
                                 output directory. Please see example usage below.
OPTIONS:
  -o,  --outdir  [Type: Path]  Path to output directory. If not provided, the Path
                                will default to the current working directory of
                                this script [Default: $(dirname  "$0")]
-c,  --cache  [Type: Path]     Path to singularity cache. If not provided, the Path
                                will default to the current working directory of
                                this script [Default: $(dirname  "$0")/.singularity/]
  -h, --help     [Type: Bool]  Displays usage and help information.
Example:
  $ builder slurm -h
  $ builder slurm -j RNA-seek_hg38 -b "/scratch/$USER/rawdata,/scratch/$USER/RNA_hg38,/data/CCBR_Pipeliner/db/PipeDB/,/lscratch,/fdb"
Version:
  0.1.0
EOF
}


# Functions
function err() { cat <<< "$@" 1>&2; }
function fatal() { cat <<< "$@" 1>&2; usage; exit 1; }
function abspath() { readlink -e "$1"; }
function parser() {
  # Adds parsed command-line args to GLOBAL $Arguments associative array
  # + KEYS = short_cli_flag ("j", "o", ...)
  # + VALUES = parsed_user_value ("MasterJobName" "/scratch/hg38", ...)
  # @INPUT "$@" = user command-line arguments
  # @CALLS check() to see if the user provided all the required arguments

  while [[ $# -gt 0 ]]; do
    key="$1"
    case $key in
      -h  | --help) usage && exit 0;;
      -j  | --job-name)   provided "$key" "${2:-}"; Arguments["j"]="$2"; shift; shift;;
      -b  | --bind-paths) provided "$key" "${2:-}"; Arguments["b"]="$2"; shift; shift;;
      -o  | --outdir)  provided "$key" "${2:-}"; Arguments["o"]="$2"; shift; shift;;
      -c  | --cache)  provided "$key" "${2:-}"; Arguments["c"]="$2"; shift; shift;;
      -*  | --*) err "Error: Failed to parse unsupported argument: '${key}'."; usage && exit 1;;
      *) err "Error: Failed to parse unrecognized argument: '${key}'. Do any of your inputs have spaces?"; usage && exit 1;;
    esac
  done

  # Check for required args
  check
}


function provided() {
  # Checks to see if the argument's value exists
  # @INPUT $1 = name of user provided argument
  # @INPUT $2 = value of user provided argument
  # @CALLS fatal() if value is empty string or NULL

  if [[ -z "${2:-}" ]]; then
     fatal "Fatal: Failed to provide value to '${1}'!";
  fi
}


function check(){
  # Checks to see if user provided required arguments
  # @INPUTS $Arguments = Global Associative Array
  # @CALLS fatal() if user did NOT provide all the $required args

  # List of required arguments
  local required=("j" "b")
  #echo -e "Provided Required Inputs"
  for arg in "${required[@]}"; do
    value=${Arguments[${arg}]:-}
    if [[ -z "${value}" ]]; then
      fatal "Failed to provide all required args.. missing ${arg}"
    fi
  done
}


function submit(){
  # Submit jobs to the defined job scheduler or executor (i.e. slurm)
  # INPUT $1 = Snakemake Mode of execution
  # INPUT $2 = Name of master/main job or process (pipeline controller)
  # INPUT $3 = Pipeline output directory
  # INPUT $4 = Singularity Bind paths
  # INPUT $5 = Singularity cache directory

  # SLURM inherits the environment from which the job was launched
  # Try to purge modules all modules from environment
  command -V module &> /dev/null && module purge

  # Check if singularity in $PATH
  # If not, try to module load singularity as a last resort
  command -V singularity &> /dev/null || module load singularity || \
    fatal "Fail to find or load 'singularity', not installed on target system."

  # Check if snakemake in $PATH
  # If not, try to module load snakemake as a last resort
  command -V snakemake &> /dev/null || module load snakemake || \
    fatal "Fail to find or load 'snakemake', not installed on target system."

  # Snakemake executor
  executor=${1}

  # Goto Pipeline Ouput directory
  # Create a local singularity cache in output directory
  # cache can be re-used instead of re-pulling from DockerHub everytime
  cd "$3" && export SINGULARITY_CACHEDIR="${5}"

  # unsetting XDG_RUNTIME_DIR to avoid some unsighly but harmless warnings
  unset XDG_RUNTIME_DIR

  # Run the workflow with specified executor
  case "$executor" in
    slurm)
          # Create directory for logfiles
          mkdir -p "$3"/logfiles/slurmfiles/
          # Submit the master job to the cluster
          job_id=$(sbatch --parsable -J "$2" --gres=lscratch:100 --time=4-00:00:00 --mail-type=BEGIN,END,FAIL \
          --output "$3"/logfiles/snakemake.log --error "$3"/logfiles/snakemake.log \
          snakemake --latency-wait 120 -s "$3"/workflow/rules/build.smk -d "$3" \
            --use-singularity --singularity-args "'-B $4'" --configfile="$3"/config/build.yml \
            --printshellcmds --cluster-config "$3"/resources/build_cluster.json \
            --cluster "sbatch --gres {cluster.gres} --cpus-per-task {cluster.threads} -p {cluster.partition} -t {cluster.time} --mem {cluster.mem} --job-name={params.rname}" \
            --keep-going --restart-times 3 -j 500 --rerun-incomplete --stats "$3"/logfiles/runtime_statistics.json \
            --keep-remote 2>&1 | tee -a "$3"/logfiles/master.log)
          # Clean-up working directory after completion
          cleanup_id=$(sbatch -J "pl:clean" --dependency=afterany:${job_id} "$3"/resources/cleanup "$3" "$job_id")
        ;;
      *)  echo "${executor} is not available." && \
          fatal "Failed to provide valid execution backend: ${executor}. Please use slurm."
        ;;
    esac

  # Return exit-code of pipeline sumbission
  echo "$job_id"
}


function main(){
  # Parses args and runs initializes output directory, setups cromwell, and runs pipeline
  # @INPUT "$@" = command-line arguments
  # @CALLS parser(), initialize(), setup(), cromwell()

  if [ $# -eq 0 ]; then usage; exit 1; fi

  # Associative array to store parsed args
  declare -Ag Arguments

  # Positional Argument for Snakemake Executor
  case $1 in
    slurm) Arguments["e"]="$1";;
    -h    | --help | help) usage && exit 0;;
    -*    | --*) err "Error: Failed to provide required positional argument: <slurm>."; usage && exit 1;;
    *) err "Error: Failed to provide valid positional argument. '${1}' is not supported. Valid option(s) are slurm"; usage && exit 1;;
  esac

  # Parses remaining user provided command-line arguments
  parser "${@:2}" # Remove first item of list

  # Setting defaults for non-required arguments
  # If output directory is not provided, default to scripts current working directory
  outdir=$(abspath $(dirname  "$0"))
  Arguments[o]="${Arguments[o]:-$outdir}"
  Arguments[o]="${Arguments[o]%/}" # clean outdir path (remove trailing '/')

  # If singularity cache not provided, default to ${outdir}/.singularity
  cache="${Arguments[o]}/.singularity"
  Arguments[c]="${Arguments[c]:-$cache}"
  Arguments[c]="${Arguments[c]%/}" # clean outdir path (remove trailing '/')

  # Print pipeline metadata prior to running
  echo -e "\nRNA-seek Builder\t$(date)"
  echo -e "Running pipeline with the following parameters:"
  for key in "${!Arguments[@]}"; do echo -e "\t${key}\t${Arguments["$key"]}"; done

  # Step 1. Run builder pipeline and submit jobs to cluster using the defined executor
  job_id=$(submit "${Arguments[e]}" "${Arguments[j]}" "${Arguments[o]}" "${Arguments[b]}" "${Arguments[c]}")
  echo -e "RNA-seek build reference pipeline submitted to cluster.\nMaster Job ID: $job_id"

  # Step 2. Clean-up steps after completes (To be implemented)
  # clean # add post-processing batch script which depends on completion of job id
}


# Main: check usage, parse args, and run pipeline
main "$@"
